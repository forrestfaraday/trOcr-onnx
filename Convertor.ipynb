{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/forest/miniconda3/envs/ort/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import OR\n",
    "from optimum.onnxruntime import ORTOptimizer, ORTModelForTokenClassification, OptimizationConfig\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "from optimum.onnxruntime import ORTQuantizer\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Converter:\n",
    "    def __init__(self, model_name):\n",
    "        self.model = ORTModelForTokenClassification.from_pretrained(model_name, from_transformers=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    def onnxConvertor(self):\n",
    "        self.model.save_pretrained(\"onnx_model\")\n",
    "        self.tokenizer.save_pretrained(\"onnx_model\")\n",
    "\n",
    "    def optimizer_for_gpu(self):\n",
    "        ort_model = ORTModelForTokenClassification.from_pretrained('onnx_model/', file_name = 'model.onnx', provider='CUDAExecutionProvider')\n",
    "        optimizer = ORTOptimizer.from_pretrained(ort_model)\n",
    "        optimization_config = OptimizationConfig(optimization_level=2, # full optimize\n",
    "                                                optimize_for_gpu=True, # use CUDAExecutionProvider\n",
    "                                                fp16=True\n",
    "                                                )\n",
    "        optimizer.optimize(save_dir='optimized_model_for_gpu', optimization_config=optimization_config)\n",
    "        self.tokenizer.save_pretrained('optimized_model_for_gpu/')\n",
    "    \n",
    "    def optimizer_for_cpu_fp16(self):\n",
    "        ort_model = ORTModelForTokenClassification.from_pretrained('onnx_model/', file_name = 'model.onnx', provider='CPUExecutionProvider')\n",
    "        optimizer = ORTOptimizer.from_pretrained(ort_model)\n",
    "        optimization_config = OptimizationConfig(optimization_level=2, # 99 full optimize\n",
    "                                                optimize_for_gpu=False, # use CPUExecutionProvider\n",
    "                                                fp16=True\n",
    "                                                )\n",
    "        optimizer.optimize(save_dir='optimized_model_for_cpu_fp16', optimization_config=optimization_config)\n",
    "        self.tokenizer.save_pretrained('optimized_model_for_cpu_fp16/')\n",
    "    \n",
    "    def optimizer_for_cpu(self):\n",
    "        ort_model = ORTModelForTokenClassification.from_pretrained('onnx_model/', file_name = 'model.onnx', provider='CPUExecutionProvider')\n",
    "        optimizer = ORTOptimizer.from_pretrained(ort_model)\n",
    "        optimization_config = OptimizationConfig(optimization_level=2, # 99 full optimize\n",
    "                                                optimize_for_gpu=False, # use CPUExecutionProvider\n",
    "                                                fp16=False\n",
    "                                                )\n",
    "        optimizer.optimize(save_dir='optimized_model_for_cpu', optimization_config=optimization_config)\n",
    "        self.tokenizer.save_pretrained('optimized_model_for_cpu/')    \n",
    "    \n",
    "    def optimized_quantized(self,ort_model_name):\n",
    "        ort_model = ORTModelForTokenClassification.from_pretrained(ort_model_name, file_name = 'model_optimized.onnx', provider='CPUExecutionProvider')\n",
    "        qconfig = AutoQuantizationConfig.arm64(is_static=False, per_channel=False)\n",
    "        quantizer = ORTQuantizer.from_pretrained(ort_model)\n",
    "        # Apply dynamic quantization on the model\n",
    "        quantizer.quantize(save_dir='dynamic_quantized_model', quantization_config=qconfig)\n",
    "        self.tokenizer.save_pretrained('dynamic_quantized_model/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = Converter(\"/home/forest/Desktop/ML-HUB/nlp-health-deidentification-sub-base-ro/nioyatech/nlp-health-deidentification-sub-base-ro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.onnxConvertor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.optimizer_for_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.optimizer_for_cpu_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.optimizer_for_cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.optimized_quantized('optimized_model_for_cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ORTModelForVision2Seq' from 'optimum.onnxruntime' (/home/forest/miniconda3/envs/ort/lib/python3.9/site-packages/optimum/onnxruntime/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptimum\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39monnxruntime\u001b[39;00m \u001b[39mimport\u001b[39;00m ORTModelForVision2Seq\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ORTModelForVision2Seq' from 'optimum.onnxruntime' (/home/forest/miniconda3/envs/ort/lib/python3.9/site-packages/optimum/onnxruntime/__init__.py)"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForVision2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optimum.onnxruntime.modeling_vision2seq'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m TrOCRProcessor\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptimum\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39monnxruntime\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_vision2seq\u001b[39;00m \u001b[39mimport\u001b[39;00m ORTModelForVision2Seq\n\u001b[1;32m      4\u001b[0m processor \u001b[39m=\u001b[39m TrOCRProcessor\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mmicrosoft/trocr-base-printed\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m onnx_model \u001b[39m=\u001b[39m ORTModelForVision2Seq\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      6\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmicrosoft/trocr-base-handwritten\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m      7\u001b[0m     from_transformers\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m     use_cache\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optimum.onnxruntime.modeling_vision2seq'"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor\n",
    "from optimum.onnxruntime.modeling_vision2seq import ORTModelForVision2Seq\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-printed')\n",
    "onnx_model = ORTModelForVision2Seq.from_pretrained(\n",
    "    \"microsoft/trocr-base-handwritten\", \n",
    "    from_transformers=True,\n",
    "    use_cache=True,\n",
    ")\n",
    "pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "generated_ids = onnx_model.generate(pixel_values)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Framework not requested. Using torch to export to ONNX.\n",
      "Downloading: 100%|█████████████████████████| 4.13k/4.13k [00:00<00:00, 5.49MB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.33G/1.33G [05:36<00:00, 3.97MB/s]\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading: 100%|██████████████████████████████| 228/228 [00:00<00:00, 285kB/s]\n",
      "Using framework PyTorch: 1.13.0+cu117\n",
      "/home/forest/miniconda3/envs/ort/lib/python3.9/site-packages/transformers/models/vit/modeling_vit.py:176: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if num_channels != self.num_channels:\n",
      "/home/forest/miniconda3/envs/ort/lib/python3.9/site-packages/transformers/models/vit/modeling_vit.py:181: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if height != self.image_size[0] or width != self.image_size[1]:\n",
      "Validating ONNX model...\n",
      "\t-[✓] ONNX model output names match reference model ({'last_hidden_state'})\n",
      "\t- Validating ONNX Model output \"last_hidden_state\":\n",
      "\t\t-[✓] (3, 577, 768) matches (3, 577, 768)\n",
      "\t\t-[x] values not close enough (atol: 0.001)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/forest/miniconda3/envs/ort/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/forest/miniconda3/envs/ort/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/forest/miniconda3/envs/ort/lib/python3.9/site-packages/transformers/onnx/__main__.py\", line 180, in <module>\n",
      "    main()\n",
      "  File \"/home/forest/miniconda3/envs/ort/lib/python3.9/site-packages/transformers/onnx/__main__.py\", line 107, in main\n",
      "    validate_model_outputs(\n",
      "  File \"/home/forest/miniconda3/envs/ort/lib/python3.9/site-packages/transformers/onnx/convert.py\", line 472, in validate_model_outputs\n",
      "    raise ValueError(\n",
      "ValueError: Outputs values doesn't match between reference model and ONNX exported model: Got max absolute difference of: 0.0010647773742675781 for [-5.4995537] vs [-5.5006104]\n"
     ]
    }
   ],
   "source": [
    "model_ckpt = \"microsoft/trocr-base-handwritten\"\n",
    "!python -m transformers.onnx --model={model_ckpt} --feature=vision2seq-lm onnx/ --atol 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers.models.vision2seq'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39monnx\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, AutoModelForVision2Seq\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvision2seq\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_vision2seq\u001b[39;00m \u001b[39mimport\u001b[39;00m Vision2SeqConfig\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvision2seq\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_vision2seq\u001b[39;00m \u001b[39mimport\u001b[39;00m Vision2SeqForConditionalGeneration   \n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvision2seq\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_vision2seq\u001b[39;00m \u001b[39mimport\u001b[39;00m Vision2SeqForImageCaptioning\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers.models.vision2seq'"
     ]
    }
   ],
   "source": [
    "# export vision2seq model to onnx\n",
    "import onnx\n",
    "from transformers import AutoTokenizer, AutoModelForVision2Seq\n",
    "from transformers.models.vision2seq.modeling_vision2seq import Vision2SeqConfig\n",
    "from transformers.models.vision2seq.modeling_vision2seq import Vision2SeqForConditionalGeneration   \n",
    "from transformers.models.vision2seq.modeling_vision2seq import Vision2SeqForImageCaptioning\n",
    "from transformers.models.vision2seq.modeling_vision2seq import Vision2SeqForImageClassification\n",
    "from transformers.models.vision2seq.modeling_vision2seq import Vision2SeqForImageToTextGeneration\n",
    "from transformers.models.vision2seq.modeling_vision2seq import Vision2SeqForImageToTextRetrieval\n",
    "from transformers.models.vision2seq.modeling_vision2seq import Vision2SeqForImageToTextTranslation\n",
    "from transformers.models.vision2seq.modeling_vision2seq import Vision2SeqForImageTranslation\n",
    "from transformers.models.vision2seq.modeling_vision2seq import Vision2SeqForImageToTextTranslation\n",
    "from transformers.models.vision2seq.modeling_vision2seq import Vision2SeqForImageToTextTranslation\n",
    "from transformers.models.vision2seq.modeling_vision2seq import Vision2SeqForImageToTextTranslation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.vision2seq.modeling_vision2seq import Vision2SeqForImageToTextTranslation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ort",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c3e05ff27fc98a91ae76f0551d628de3e89010ccf719267306f9ed11c6df735"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
