{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b08ad73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f44fbe6af5490aadec54bbdd0ea936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38881ed8be8a450c947dfe2a7b8b8a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db6313e06ae42c1a0b3314bc15a0afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f7c425c98de4eadaaa6382b4083e045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4caa13dcc71545788eaf7df47fdf554c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12acaf255c61449097d4508fad1692ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e16043206a64a8488d80f20b3106e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.23G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel, TrOCRProcessor\n",
    "model_name = \"microsoft/trocr-large-handwritten\"\n",
    "processor = TrOCRProcessor.from_pretrained(model_name)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "684a2e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Framework not requested. Using torch to export to ONNX.\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using framework PyTorch: 1.13.0+cu117\n",
      "/home/forest/miniconda3/envs/ort/lib/python3.9/site-packages/transformers/models/vit/modeling_vit.py:176: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if num_channels != self.num_channels:\n",
      "/home/forest/miniconda3/envs/ort/lib/python3.9/site-packages/transformers/models/vit/modeling_vit.py:181: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if height != self.image_size[0] or width != self.image_size[1]:\n",
      "Validating ONNX model...\n",
      "\t-[✓] ONNX model output names match reference model ({'last_hidden_state'})\n",
      "\t- Validating ONNX Model output \"last_hidden_state\":\n",
      "\t\t-[✓] (3, 577, 768) matches (3, 577, 768)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "Using framework PyTorch: 1.13.0+cu117\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "/home/forest/miniconda3/envs/ort/lib/python3.9/site-packages/transformers/models/trocr/modeling_trocr.py:526: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1:\n",
      "/home/forest/miniconda3/envs/ort/lib/python3.9/site-packages/transformers/models/trocr/modeling_trocr.py:53: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min))\n",
      "/home/forest/miniconda3/envs/ort/lib/python3.9/site-packages/transformers/models/trocr/modeling_trocr.py:258: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/home/forest/miniconda3/envs/ort/lib/python3.9/site-packages/transformers/models/trocr/modeling_trocr.py:265: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "/home/forest/miniconda3/envs/ort/lib/python3.9/site-packages/transformers/models/trocr/modeling_trocr.py:297: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "Validating ONNX model...\n",
      "\t-[✓] ONNX model output names match reference model ({'logits'})\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[✓] (3, 9, 50265) matches (3, 9, 50265)\n",
      "\t\t-[✓] all values close (atol: 0.001)\n",
      "All good, model saved at: onnx/encoder_model.onnx, onnx/decoder_model.onnx\n"
     ]
    }
   ],
   "source": [
    "!python -m transformers.onnx --model={model_name} --feature=vision2seq-lm onnx/ --atol 1e-3 --preprocessor=processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12943cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: Hugging Face Transformers ONNX exporter [-h] -m MODEL\n",
      "                                               [--feature {causal-lm,causal-lm-with-past,default,default-with-past,image-classification,image-segmentation,masked-im,masked-lm,multiple-choice,object-detection,question-answering,semantic-segmentation,seq2seq-lm,seq2seq-lm-with-past,sequence-classification,speech2seq-lm,speech2seq-lm-with-past,token-classification,vision2seq-lm}]\n",
      "                                               [--opset OPSET] [--atol ATOL]\n",
      "                                               [--framework {pt,tf}]\n",
      "                                               [--cache_dir CACHE_DIR]\n",
      "                                               [--preprocessor {auto,tokenizer,feature_extractor,processor}]\n",
      "                                               output\n",
      "\n",
      "positional arguments:\n",
      "  output                Path indicating where to store generated ONNX model.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -m MODEL, --model MODEL\n",
      "                        Model ID on huggingface.co or path on disk to load\n",
      "                        model from.\n",
      "  --feature {causal-lm,causal-lm-with-past,default,default-with-past,image-classification,image-segmentation,masked-im,masked-lm,multiple-choice,object-detection,question-answering,semantic-segmentation,seq2seq-lm,seq2seq-lm-with-past,sequence-classification,speech2seq-lm,speech2seq-lm-with-past,token-classification,vision2seq-lm}\n",
      "                        The type of features to export the model with.\n",
      "  --opset OPSET         ONNX opset version to export the model with.\n",
      "  --atol ATOL           Absolute difference tolerance when validating the\n",
      "                        model.\n",
      "  --framework {pt,tf}   The framework to use for the ONNX export. If not\n",
      "                        provided, will attempt to use the local checkpoint's\n",
      "                        original framework or what is available in the\n",
      "                        environment.\n",
      "  --cache_dir CACHE_DIR\n",
      "                        Path indicating where to store cache.\n",
      "  --preprocessor {auto,tokenizer,feature_extractor,processor}\n",
      "                        Which type of preprocessor to use. 'auto' tries to\n",
      "                        automatically detect it.\n"
     ]
    }
   ],
   "source": [
    "!python -m transformers.onnx --help\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626fd0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "3c3e05ff27fc98a91ae76f0551d628de3e89010ccf719267306f9ed11c6df735"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
