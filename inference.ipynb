{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cad3a736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from onnx_trocr_inference import *\n",
    "import onnxruntime as onnxrt\n",
    "import requests\n",
    "from transformers import AutoConfig, AutoModelForVision2Seq, TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from transformers.generation.utils import GenerationMixin\n",
    "from transformers.modeling_outputs import BaseModelOutput, Seq2SeqLMOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff10bd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "\n",
    "model_name = \"/home/forest/Desktop/trOcr-onnx/ocr_model\"\n",
    "processor = TrOCRProcessor.from_pretrained(model_name)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
    "\n",
    "# load image from the IAM dataset\n",
    "url = \"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTy8XVkFVffEiSZLjWFgn_ZDm3JF55TglkUgQ&usqp=CAU \"\n",
    "image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "pixel_values = processor([image], return_tensors=\"pt\").pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b616975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ort():\n",
    "    processor = TrOCRProcessor.from_pretrained(model_name)\n",
    "    model = ORTModelForVision2Seq()\n",
    "    model = model.to(device)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    model.config.decoder_start_token_id = 2\n",
    "    model.config.vocab_size = model.config.decoder.vocab_size\n",
    "    model.config.pad_token_id = model.config.decoder.pad_token_id = processor.tokenizer.pad_token_id\n",
    "    model.config.eos_token_id = model.config.decoder.eos_token_id = processor.tokenizer.sep_token_id\n",
    "\n",
    "    generated_ids = model.generate(pixel_values.to(device))\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    model_output = processor.tokenizer.batch_decode(generated_ids, skip_special_tokens=True, device=device)[0]\n",
    "\n",
    "    print(\"ORT time: \", end - start, model_output)\n",
    "\n",
    "\n",
    "def test_original():\n",
    "    processor = TrOCRProcessor.from_pretrained(model_name)\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
    "\n",
    "    start = time.time()\n",
    "    generated_ids = model.generate(pixel_values.to(device))\n",
    "    end = time.time()\n",
    "\n",
    "    model_output = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    print(\"Original time: \", end - start, model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da43739d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_ort()\n",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m, in \u001b[0;36mtest_ort\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mpad_token_id \u001b[39m=\u001b[39m processor\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mpad_token_id\n\u001b[1;32m     11\u001b[0m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39meos_token_id \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39meos_token_id \u001b[39m=\u001b[39m processor\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39msep_token_id\n\u001b[0;32m---> 13\u001b[0m generated_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(pixel_values\u001b[39m.\u001b[39;49mto(device))\n\u001b[1;32m     15\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     17\u001b[0m model_output \u001b[39m=\u001b[39m processor\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mbatch_decode(generated_ids, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, device\u001b[39m=\u001b[39mdevice)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/ort/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ort/lib/python3.9/site-packages/transformers/generation/utils.py:1367\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, penalty_alpha, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m   1360\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1361\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgeneration results, please set `padding_side=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m\u001b[39m` when initializing the tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1362\u001b[0m         )\n\u001b[1;32m   1364\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m model_kwargs:\n\u001b[1;32m   1365\u001b[0m     \u001b[39m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[1;32m   1366\u001b[0m     \u001b[39m# and added to `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 1367\u001b[0m     model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_encoder_decoder_kwargs_for_generation(\n\u001b[1;32m   1368\u001b[0m         inputs_tensor, model_kwargs, model_input_name\n\u001b[1;32m   1369\u001b[0m     )\n\u001b[1;32m   1371\u001b[0m \u001b[39m# 4. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[1;32m   1372\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m~/miniconda3/envs/ort/lib/python3.9/site-packages/transformers/generation/utils.py:601\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[1;32m    599\u001b[0m encoder_kwargs[\u001b[39m\"\u001b[39m\u001b[39mreturn_dict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    600\u001b[0m encoder_kwargs[model_input_name] \u001b[39m=\u001b[39m inputs_tensor\n\u001b[0;32m--> 601\u001b[0m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m]: ModelOutput \u001b[39m=\u001b[39m encoder(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mencoder_kwargs)\n\u001b[1;32m    603\u001b[0m \u001b[39mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/ort/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/trOcr-onnx/onnx_trocr_inference.py:40\u001b[0m, in \u001b[0;36mORTEncoder.forward\u001b[0;34m(self, pixel_values, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m# Run inference\u001b[39;00m\n\u001b[1;32m     39\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession\u001b[39m.\u001b[39mrun(\u001b[39mNone\u001b[39;00m, onnx_inputs)\n\u001b[0;32m---> 40\u001b[0m last_hidden_state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(outputs[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_names[\u001b[39m\"\u001b[39;49m\u001b[39mlast_hidden_state\u001b[39;49m\u001b[39m\"\u001b[39;49m]])\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_device)\n\u001b[1;32m     42\u001b[0m \u001b[39mreturn\u001b[39;00m BaseModelOutput(last_hidden_state\u001b[39m=\u001b[39mlast_hidden_state)\n",
      "File \u001b[0;32m~/miniconda3/envs/ort/lib/python3.9/site-packages/torch/cuda/__init__.py:229\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n\u001b[1;32m    228\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mLAZY\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 229\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    230\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    233\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero."
     ]
    }
   ],
   "source": [
    "test_ort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27946279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854c74cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "3c3e05ff27fc98a91ae76f0551d628de3e89010ccf719267306f9ed11c6df735"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
